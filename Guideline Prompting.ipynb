{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1aa3b3-4a51-4d18-8779-93b96f19d795",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "import streamlit as st\n",
    "import LangChainInterface\n",
    "from watsonxlangchain import LangChainInterface\n",
    "\n",
    "# IBM WatsonX credentials\n",
    "creds = {\n",
    "    'apikey': 'ZNPRw05jup0f2SFOCFkAtfZskeCPL6fESGaYAq1EWRq7',\n",
    "    'url': 'https://us-south.ml.cloud.ibm.com'\n",
    "}\n",
    "\n",
    "# Set up the LLM using IBM WatsonX\n",
    "llm = LangChainInterface(\n",
    "    credentials=creds,\n",
    "    model='meta-llama/llama-2-70b-chat',\n",
    "    params={\n",
    "        'decoding_method': 'sample',\n",
    "        'max_new_tokens': 200,\n",
    "        'temperature': 0.5\n",
    "    },\n",
    "    project_id='dd58941e-28cc-4abe-9189-0bebc2f2edec'\n",
    ")\n",
    "\n",
    "# Load and index the PDF with caching\n",
    "@st.cache_resource\n",
    "def load_pdf():\n",
    "    pdf_name = 'what is generative ai.pdf'  # fixed assignment syntax\n",
    "    loaders = [PyPDFLoader(pdf_name)]\n",
    "    index = VectorstoreIndexCreator(\n",
    "        embedding=HuggingFaceEmbeddings(model_name='all-MiniLM-L12-v2'),\n",
    "        text_splitter=RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=10)  # fixed typo: nk_overlap\n",
    "    ).from_loaders(loaders)\n",
    "    return index\n",
    "\n",
    "index = load_pdf()\n",
    "\n",
    "# Build the retrieval-based question-answering chain\n",
    "chain = RetrievalQA.from_chain_type(  # fixed typo: `rom_chain_type` â†’ `from_chain_type`\n",
    "    llm=llm,\n",
    "    chain_type='stuff',\n",
    "    retriever=index.vectorstore.as_retriever(),\n",
    "    input_key='question'\n",
    ")\n",
    "\n",
    "# Streamlit UI\n",
    "st.title('Medicine Chatbot')\n",
    "\n",
    "# Initialize message history\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "\n",
    "# Display chat history\n",
    "for message in st.session_state.messages:\n",
    "    st.chat_message(message['role']).markdown(message['content'])\n",
    "\n",
    "# Accept user input\n",
    "prompt = st.chat_input('Pass Your Prompt here')\n",
    "\n",
    "if prompt:\n",
    "    st.chat_message('user').markdown(prompt)\n",
    "    st.session_state.messages.append({'role': 'user', 'content': prompt})\n",
    "\n",
    "    response = chain.run(prompt)\n",
    "\n",
    "    st.chat_message('assistant').markdown(response)\n",
    "    st.session_state.messages.append({'role': 'assistant', 'content': response})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
